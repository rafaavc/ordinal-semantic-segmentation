{
  "activation": "OrdinalEncoding",
  "batch_size": 16,
  "chosen_loss": 1.4717772006988525,
  "dataset": "KelwinTeethISBI",
  "dataset_mask_type": "na",
  "dataset_scale": 1.0,
  "folds": {
    "0": {
      "loss": 1.4717772006988525,
      "n_epochs": 195
    },
    "1": {
      "loss": 1.6930592060089111,
      "n_epochs": 198
    },
    "2": {
      "loss": 1.6714826822280884,
      "n_epochs": 200
    },
    "3": {
      "loss": 1.7069298028945923,
      "n_epochs": 200
    },
    "4": {
      "loss": 1.7293057441711426,
      "n_epochs": 188
    }
  },
  "k": 5,
  "log_interval": 5,
  "loss": "CrossEntropyProbs",
  "lr": 0.0001,
  "lr_fac": 0.1,
  "max_epochs": 200,
  "model": "UNet",
  "model_name": "UNet_final_kelwin/UNet_KelwinTeethISBI1.0_na_OrdinalEncoding_CrossEntropyProbs_1.0_200e_3e36c3c181_",
  "num_classes": 5,
  "only_first_fold": false,
  "patience": 100,
  "pretrained": false,
  "refinement": 1,
  "regularization_weight": 1.0,
  "seed": 1,
  "semisupervised": false,
  "training_duration_per_fold_s": 2933.539877653122,
  "use_cuda": true
}